{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "\n",
    "# Data Gathering\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "#### This Notebook\n",
    "- [Scraping Function](#func)\n",
    "- [Scraping the Subreddits](#scrape)\n",
    "\n",
    "#### Other Notebooks\n",
    "- [Data Cleaning and EDA](cleaning_and_EDA.ipynb)\n",
    "- [Models](models.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"func\"></a>\n",
    "\n",
    "## Scraping Function\n",
    "\n",
    "---\n",
    "\n",
    "Factorizing the scraping process into a function makes scraping fast and repeatable. This allows us to either gather a lot of data from a few subreddits or lots of data from a variety of subreddits. In this project, data was scraped at different days of the week after posts have had a chance to cycle through. This is more important if the chosen subreddit is slower or has less subscribers.\n",
    "\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for scraping a specified subreddit for n amount of posts.\n",
    "# Calling the API for a specific subreddit without an \"after\" query\n",
    "# will call the first 25 posts in addition to any pinned posts, so\n",
    "# it is likely that the returned amount of posts will be slightly \n",
    "# higher than the given amount by a few. \n",
    "\n",
    "# After around 1000 posts, reddit's API will start to give duplicate posts.\n",
    "# Duplicate checking is the duty of the user, especially if scraping again \n",
    "# before new posts have had a chance to circulate through the subreddit.\n",
    "\n",
    "def scrape_subreddit(subreddit, n_posts, u_agent = \"pepega bot\", nap = 2):\n",
    "    \n",
    "    posts = []\n",
    "    params = {}\n",
    "    \n",
    "    for i in range(0, n_posts, 25):\n",
    "        \n",
    "        print(f\"Gathering {i + 25} {str(subreddit)} posts...\", end = \"\")\n",
    "\n",
    "        url = f\"https://www.reddit.com/r/{str(subreddit)}.json\"\n",
    "        res = requests.get(url, params = params, headers = {\"User-agent\": u_agent})\n",
    "        \n",
    "        if res.status_code != 200:\n",
    "            print(\"\\n\" + f\"Unexpected status code, exiting loop. Status code: {res.status_code}\")\n",
    "            break\n",
    "        \n",
    "        json = res.json()\n",
    "        posts.extend(json[\"data\"][\"children\"])\n",
    "        params = {\"after\": json[\"data\"][\"after\"]}\n",
    "\n",
    "        time.sleep(nap)\n",
    "        \n",
    "        print(\"Complete!\")\n",
    "    \n",
    "    # automatically \"de-nest\" the posts; \"kind\" key is useless for our purposes\n",
    "    for i in range(len(posts)):\n",
    "        posts[i] = posts[i][\"data\"]\n",
    "        \n",
    "    print(f\"Scrape complete, returning {len(posts)} posts.\")\n",
    "        \n",
    "    return posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"scrape\"></a>\n",
    "## Scraping the Subreddits\n",
    "---\n",
    "\n",
    "This is where the scraping itself takes place. First, the basic training set of about 1000 posts each is gathered. These are later commented out so they cannot be rerun, so that the training data does not change later. Having consistent training data is important for interpretations of the models that will take place in the Models notebook. Later, testing data is scraped about a week later. This is in the interest of mimicking a Kaggle competition, wherein the model is built on training data and the true evaluation comes from how well the model performs on completely unseen data. Finally, posts are scraped from similar subreddits that have the same topics. This is to that the model can classify the underlying topics of the subreddit, rather than trying to classify the particulars of a single subreddit, such as the links shared or automated threads generated by the auto moderators.  \n",
    "\n",
    "[Back to Top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering 25 DNDNext posts...Complete!\n",
      "Gathering 50 DNDNext posts...Complete!\n",
      "Gathering 75 DNDNext posts...Complete!\n",
      "Gathering 100 DNDNext posts...Complete!\n",
      "Gathering 125 DNDNext posts...Complete!\n",
      "Gathering 150 DNDNext posts...Complete!\n",
      "Gathering 175 DNDNext posts...Complete!\n",
      "Gathering 200 DNDNext posts...Complete!\n",
      "Gathering 225 DNDNext posts...Complete!\n",
      "Gathering 250 DNDNext posts...Complete!\n",
      "Gathering 275 DNDNext posts...Complete!\n",
      "Gathering 300 DNDNext posts...Complete!\n",
      "Gathering 325 DNDNext posts...Complete!\n",
      "Gathering 350 DNDNext posts...Complete!\n",
      "Gathering 375 DNDNext posts...Complete!\n",
      "Gathering 400 DNDNext posts...Complete!\n",
      "Gathering 425 DNDNext posts...Complete!\n",
      "Gathering 450 DNDNext posts...Complete!\n",
      "Gathering 475 DNDNext posts...Complete!\n",
      "Gathering 500 DNDNext posts...Complete!\n",
      "Gathering 525 DNDNext posts...Complete!\n",
      "Gathering 550 DNDNext posts...Complete!\n",
      "Gathering 575 DNDNext posts...Complete!\n",
      "Gathering 600 DNDNext posts...Complete!\n",
      "Gathering 625 DNDNext posts...Complete!\n",
      "Gathering 650 DNDNext posts...Complete!\n",
      "Gathering 675 DNDNext posts...Complete!\n",
      "Gathering 700 DNDNext posts...Complete!\n",
      "Gathering 725 DNDNext posts...Complete!\n",
      "Gathering 750 DNDNext posts...Complete!\n",
      "Gathering 775 DNDNext posts...Complete!\n",
      "Gathering 800 DNDNext posts...Complete!\n",
      "Gathering 825 DNDNext posts...Complete!\n",
      "Gathering 850 DNDNext posts...Complete!\n",
      "Gathering 875 DNDNext posts...Complete!\n",
      "Gathering 900 DNDNext posts...Complete!\n",
      "Gathering 925 DNDNext posts...Complete!\n",
      "Gathering 950 DNDNext posts...Complete!\n",
      "Gathering 975 DNDNext posts...Complete!\n",
      "Gathering 1000 DNDNext posts...Complete!\n",
      "Scrape complete, returning 991 posts.\n"
     ]
    }
   ],
   "source": [
    "# # Scraping the DND subreddit for the training data frame.\n",
    "# # Do not rerun!\n",
    "\n",
    "# dnd_posts = scrape_subreddit(\"DNDNext\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering 25 Pathfinder_RPG posts...Complete!\n",
      "Gathering 50 Pathfinder_RPG posts...Complete!\n",
      "Gathering 75 Pathfinder_RPG posts...Complete!\n",
      "Gathering 100 Pathfinder_RPG posts...Complete!\n",
      "Gathering 125 Pathfinder_RPG posts...Complete!\n",
      "Gathering 150 Pathfinder_RPG posts...Complete!\n",
      "Gathering 175 Pathfinder_RPG posts...Complete!\n",
      "Gathering 200 Pathfinder_RPG posts...Complete!\n",
      "Gathering 225 Pathfinder_RPG posts...Complete!\n",
      "Gathering 250 Pathfinder_RPG posts...Complete!\n",
      "Gathering 275 Pathfinder_RPG posts...Complete!\n",
      "Gathering 300 Pathfinder_RPG posts...Complete!\n",
      "Gathering 325 Pathfinder_RPG posts...Complete!\n",
      "Gathering 350 Pathfinder_RPG posts...Complete!\n",
      "Gathering 375 Pathfinder_RPG posts...Complete!\n",
      "Gathering 400 Pathfinder_RPG posts...Complete!\n",
      "Gathering 425 Pathfinder_RPG posts...Complete!\n",
      "Gathering 450 Pathfinder_RPG posts...Complete!\n",
      "Gathering 475 Pathfinder_RPG posts...Complete!\n",
      "Gathering 500 Pathfinder_RPG posts...Complete!\n",
      "Gathering 525 Pathfinder_RPG posts...Complete!\n",
      "Gathering 550 Pathfinder_RPG posts...Complete!\n",
      "Gathering 575 Pathfinder_RPG posts...Complete!\n",
      "Gathering 600 Pathfinder_RPG posts...Complete!\n",
      "Gathering 625 Pathfinder_RPG posts...Complete!\n",
      "Gathering 650 Pathfinder_RPG posts...Complete!\n",
      "Gathering 675 Pathfinder_RPG posts...Complete!\n",
      "Gathering 700 Pathfinder_RPG posts...Complete!\n",
      "Gathering 725 Pathfinder_RPG posts...Complete!\n",
      "Gathering 750 Pathfinder_RPG posts...Complete!\n",
      "Gathering 775 Pathfinder_RPG posts...Complete!\n",
      "Gathering 800 Pathfinder_RPG posts...Complete!\n",
      "Gathering 825 Pathfinder_RPG posts...Complete!\n",
      "Gathering 850 Pathfinder_RPG posts...Complete!\n",
      "Gathering 875 Pathfinder_RPG posts...Complete!\n",
      "Gathering 900 Pathfinder_RPG posts...Complete!\n",
      "Gathering 925 Pathfinder_RPG posts...Complete!\n",
      "Gathering 950 Pathfinder_RPG posts...Complete!\n",
      "Gathering 975 Pathfinder_RPG posts...Complete!\n",
      "Gathering 1000 Pathfinder_RPG posts...Complete!\n",
      "Scrape complete, returning 990 posts.\n"
     ]
    }
   ],
   "source": [
    "# # Scraping the Pathfinder subreddit for the training data frame.\n",
    "# # Do not rerun!\n",
    "\n",
    "# path_posts = scrape_subreddit(\"Pathfinder_RPG\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Code for manually checking the titles of the posts to ensure no dupes.\n",
    "# # Don't run on large numbers of posts (obviously)\n",
    "\n",
    "# for i in range(len(dnd_posts)):\n",
    "#     print(f\"POST NUMBER {i + 1}\" + \"-\" * 100 + \"\\n\")\n",
    "#     print(dnd_posts[i][\"data\"][\"title\"])\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering 25 DNDNext posts...Complete!\n",
      "Gathering 50 DNDNext posts...Complete!\n",
      "Gathering 75 DNDNext posts...Complete!\n",
      "Gathering 100 DNDNext posts...Complete!\n",
      "Gathering 125 DNDNext posts...Complete!\n",
      "Gathering 150 DNDNext posts...Complete!\n",
      "Gathering 175 DNDNext posts...Complete!\n",
      "Gathering 200 DNDNext posts...Complete!\n",
      "Gathering 225 DNDNext posts...Complete!\n",
      "Gathering 250 DNDNext posts...Complete!\n",
      "Gathering 275 DNDNext posts...Complete!\n",
      "Gathering 300 DNDNext posts...Complete!\n",
      "Gathering 325 DNDNext posts...Complete!\n",
      "Gathering 350 DNDNext posts...Complete!\n",
      "Gathering 375 DNDNext posts...Complete!\n",
      "Gathering 400 DNDNext posts...Complete!\n",
      "Gathering 425 DNDNext posts...Complete!\n",
      "Gathering 450 DNDNext posts...Complete!\n",
      "Gathering 475 DNDNext posts...Complete!\n",
      "Gathering 500 DNDNext posts...Complete!\n",
      "Scrape complete, returning 502 posts.\n",
      "Gathering 25 Pathfinder_RPG posts...Complete!\n",
      "Gathering 50 Pathfinder_RPG posts...Complete!\n",
      "Gathering 75 Pathfinder_RPG posts...Complete!\n",
      "Gathering 100 Pathfinder_RPG posts...Complete!\n",
      "Gathering 125 Pathfinder_RPG posts...Complete!\n",
      "Gathering 150 Pathfinder_RPG posts...Complete!\n",
      "Gathering 175 Pathfinder_RPG posts...Complete!\n",
      "Gathering 200 Pathfinder_RPG posts...Complete!\n",
      "Gathering 225 Pathfinder_RPG posts...Complete!\n",
      "Gathering 250 Pathfinder_RPG posts...Complete!\n",
      "Gathering 275 Pathfinder_RPG posts...Complete!\n",
      "Gathering 300 Pathfinder_RPG posts...Complete!\n",
      "Gathering 325 Pathfinder_RPG posts...Complete!\n",
      "Gathering 350 Pathfinder_RPG posts...Complete!\n",
      "Gathering 375 Pathfinder_RPG posts...Complete!\n",
      "Gathering 400 Pathfinder_RPG posts...Complete!\n",
      "Gathering 425 Pathfinder_RPG posts...Complete!\n",
      "Gathering 450 Pathfinder_RPG posts...Complete!\n",
      "Gathering 475 Pathfinder_RPG posts...Complete!\n",
      "Gathering 500 Pathfinder_RPG posts...Complete!\n",
      "Scrape complete, returning 502 posts.\n"
     ]
    }
   ],
   "source": [
    "# Scraping for a test dataset, to be run at a later date so \n",
    "# that threads have a chance to cycle through the subreddit.\n",
    "# The purpose of this separate testing data set is to mimic\n",
    "# the structure of a Kaggle competition, such that a model\n",
    "# could be generalized to unseen data. \n",
    "\n",
    "dnd_test = scrape_subreddit(\"DNDNext\", 500)\n",
    "path_test = scrape_subreddit(\"Pathfinder_RPG\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering 25 DnDBehindTheScreen posts...Complete!\n",
      "Gathering 50 DnDBehindTheScreen posts...Complete!\n",
      "Gathering 75 DnDBehindTheScreen posts...Complete!\n",
      "Gathering 100 DnDBehindTheScreen posts...Complete!\n",
      "Gathering 125 DnDBehindTheScreen posts...Complete!\n",
      "Gathering 150 DnDBehindTheScreen posts...Complete!\n",
      "Gathering 175 DnDBehindTheScreen posts...Complete!\n",
      "Gathering 200 DnDBehindTheScreen posts...Complete!\n",
      "Gathering 225 DnDBehindTheScreen posts...Complete!\n",
      "Gathering 250 DnDBehindTheScreen posts...Complete!\n",
      "Gathering 275 DnDBehindTheScreen posts...Complete!\n",
      "Gathering 300 DnDBehindTheScreen posts...Complete!\n",
      "Gathering 325 DnDBehindTheScreen posts...Complete!\n",
      "Gathering 350 DnDBehindTheScreen posts...Complete!\n",
      "Gathering 375 DnDBehindTheScreen posts...Complete!\n",
      "Gathering 400 DnDBehindTheScreen posts...Complete!\n",
      "Gathering 425 DnDBehindTheScreen posts...Complete!\n",
      "Gathering 450 DnDBehindTheScreen posts...Complete!\n",
      "Gathering 475 DnDBehindTheScreen posts...Complete!\n",
      "Gathering 500 DnDBehindTheScreen posts...Complete!\n",
      "Scrape complete, returning 502 posts.\n",
      "Gathering 25 Pathfinder posts...Complete!\n",
      "Gathering 50 Pathfinder posts...Complete!\n",
      "Gathering 75 Pathfinder posts...Complete!\n",
      "Gathering 100 Pathfinder posts...Complete!\n",
      "Gathering 125 Pathfinder posts...Complete!\n",
      "Gathering 150 Pathfinder posts...Complete!\n",
      "Gathering 175 Pathfinder posts...Complete!\n",
      "Gathering 200 Pathfinder posts...Complete!\n",
      "Gathering 225 Pathfinder posts...Complete!\n",
      "Gathering 250 Pathfinder posts...Complete!\n",
      "Gathering 275 Pathfinder posts...Complete!\n",
      "Gathering 300 Pathfinder posts...Complete!\n",
      "Gathering 325 Pathfinder posts...Complete!\n",
      "Gathering 350 Pathfinder posts...Complete!\n",
      "Gathering 375 Pathfinder posts...Complete!\n",
      "Gathering 400 Pathfinder posts...Complete!\n",
      "Gathering 425 Pathfinder posts...Complete!\n",
      "Gathering 450 Pathfinder posts...Complete!\n",
      "Gathering 475 Pathfinder posts...Complete!\n",
      "Gathering 500 Pathfinder posts...Complete!\n",
      "Scrape complete, returning 501 posts.\n"
     ]
    }
   ],
   "source": [
    "# Scraping 500 posts from subreddits similar to the main \n",
    "# ones chosen for the project. Both subreddits are in the \n",
    "# odd position of being one of many for a their particular\n",
    "# game. For example, the DnDNext subreddit is a place for \n",
    "# discussion on specifically the newest edition of the \n",
    "# game, 5e. The actual DnD subreddit is dedicated to meta \n",
    "# discussion of DnD as a whole rather than specific\n",
    "# instances of the game being played. DnDBehindTheScreen,\n",
    "# on the other hand, is solely dedicated to discussion\n",
    "# about running the game. While confusing, this gives us a\n",
    "# unique opportunity to scrape data that is very similar in\n",
    "# terms of content but comes from a different environment.\n",
    "\n",
    "dnd_alt = scrape_subreddit(\"DnDBehindTheScreen\", 500)\n",
    "path_alt = scrape_subreddit(\"Pathfinder\", 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate Checking\n",
    "---\n",
    "\n",
    "The title is used to verify for duplicates because the post ID seemed to not be reliable, whereas every post has a different title. Actual post content cannot be used because some posts do not have body content and are null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "991\n",
      "990\n",
      "990\n",
      "990\n"
     ]
    }
   ],
   "source": [
    "print(len(dnd_posts))\n",
    "print(len(path_posts))\n",
    "\n",
    "print(len({dnd_posts[i][\"title\"] for i in range(len(dnd_posts))}))\n",
    "print(len({path_posts[i][\"title\"] for i in range(len(path_posts))}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "502\n",
      "502\n",
      "502\n",
      "502\n"
     ]
    }
   ],
   "source": [
    "print(len(dnd_test))\n",
    "print(len(path_test))\n",
    "\n",
    "print(len({dnd_test[i][\"title\"] for i in range(len(dnd_test))}))\n",
    "print(len({path_test[i][\"title\"] for i in range(len(path_test))}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dnd_posts).to_csv(\"../data/dnd_raw.csv\", index = False)\n",
    "pd.DataFrame(path_posts).to_csv(\"../data/path_raw.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dnd_test).to_csv(\"../data/dnd_test_raw.csv\", index = False)\n",
    "pd.DataFrame(path_test).to_csv(\"../data/path_test_raw.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dnd_alt).to_csv(\"../data/dnd_alt_raw.csv\", index = False)\n",
    "pd.DataFrame(path_alt).to_csv(\"../data/path_alt_raw.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "[Back to top](#top)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
